{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b25414f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a120f3",
   "metadata": {},
   "source": [
    "# Functions used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a41ee881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_wikitext(page_title):\n",
    "    \"\"\"Get raw wiki markup for a page\"\"\"\n",
    "    url = \"https://gameofthrones.fandom.com/api.php\"\n",
    "    \n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": page_title,\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"rvslots\": \"main\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        \n",
    "        # Check if page exists and has content\n",
    "        if \"revisions\" in page:\n",
    "            wikitext = page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "            return wikitext\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_wikitext_to_file(page_title, wikitext, output_dir):\n",
    "    \"\"\"Save wikitext to a file\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Clean the filename (remove invalid characters)\n",
    "    filename = page_title.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "    filename = filename.replace(\"?\", \"_\").replace(\"*\", \"_\").replace(\"|\", \"_\")\n",
    "    filename = filename.replace(\"<\", \"_\").replace(\">\", \"_\").replace('\"', \"_\")\n",
    "    filepath = os.path.join(output_dir, f\"{filename}.txt\")\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(wikitext)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def load_text_files(file: str, path: str = 'GoT_files', newLine: bool = False) -> str:\n",
    "    with open(os.path.join(path, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        if newLine:\n",
    "            text = f.read().splitlines()\n",
    "        else:\n",
    "            text = f.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "def load_graphs(graph: str, path: str = 'graphs'):\n",
    "    with open(os.path.join(path,graph), 'rb') as f:\n",
    "        loaded_graph = pickle.load(f)\n",
    "    \n",
    "    return loaded_graph\n",
    "\n",
    "def load_all_files(path: str = 'GoT_files') -> dict:\n",
    "    pages = os.listdir(path)\n",
    "\n",
    "    page_titles = [f for f in pages if not f.startswith('fetched_pages_')]\n",
    "    page_titles = [f for f in page_titles if not f.startswith('redirects_')]\n",
    "    page_titles = [f for f in page_titles if not f.startswith('failed_pages_')]\n",
    "\n",
    "    page_texts = {}\n",
    "\n",
    "    for file in pages:\n",
    "        # Skip files in Doubles subfolder and skip directories\n",
    "        if file == 'Doubles' or os.path.isdir(os.path.join(path, file)):\n",
    "            continue\n",
    "            \n",
    "        page_name =  file.replace(\".txt\", \"\")\n",
    "        text = load_text_files(file,path)\n",
    "        page_texts[page_name] = text\n",
    "    \n",
    "    return page_texts\n",
    "\n",
    "def clean_links(matches, category_map):\n",
    "    categories = []\n",
    "    links = []\n",
    "    for m in matches:\n",
    "        m = m.replace('[','').replace(']','')\n",
    "        if 'Category:' in m:\n",
    "            categories.append(m.replace('Category:','')),\n",
    "            \n",
    "        elif 'category:' in m:\n",
    "            categories.append(m.replace('category:',''))\n",
    "        else:\n",
    "            if '|' in m:\n",
    "                links.append(m.split('|')[0])\n",
    "            else:\n",
    "                links.append(m)\n",
    "    categories = clean_categories(categories,category_map)\n",
    "\n",
    "    return links, categories\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Filter out punctuation and convert to lowercase in one pass\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def check_categories(text: str, unique_categories: list = []) -> list:\n",
    "    \"\"\"Extract wiki [[Category:...]] values from raw text and append to unique_categories.\n",
    "\n",
    "    Uses a regex that handles single or double brackets and is case-insitive.\n",
    "    Returns the updated unique_categories list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Match [[Category:...]] (captures the inner content up to the first closing ]])\n",
    "    pattern = re.compile(r\"\\[\\[\\s*Category\\s*:\\s*(.+?)\\s*\\]\\]\", flags=re.IGNORECASE)\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    for m in matches:\n",
    "        cat = m.strip()\n",
    "        # normalize whitespace inside category\n",
    "        cat = ' '.join(cat.split())\n",
    "        if cat not in unique_categories:\n",
    "            unique_categories.append(cat)\n",
    "\n",
    "    return unique_categories\n",
    "\n",
    "def clean_categories(raw_categories: list, category_map: dict) -> list:\n",
    "    \"\"\"Map raw categories to normalized forms and remove duplicates while preserving order.\n",
    "    \n",
    "    Args:\n",
    "        raw_categories: List of raw category strings\n",
    "        category_map: Dictionary mapping raw -> normalized category strings\n",
    "        \n",
    "    Returns:\n",
    "        List of unique normalized category strings in order of first appearance\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    seen = set()\n",
    "    \n",
    "    for cat in raw_categories:\n",
    "        # Map to normalized form (use raw if not in mapping)\n",
    "        norm = category_map.get(cat, cat)\n",
    "        \n",
    "        # Only add if we haven't seen this normalized category yet\n",
    "        if norm not in seen:\n",
    "            normalized.append(norm)\n",
    "            seen.add(norm)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def load_category_mapping(file: str = 'category_mapping.json', path: str = 'data_handling'):\n",
    "    with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "        category_map = json.load(f)\n",
    "    \n",
    "    return category_map\n",
    "    \n",
    "\n",
    "def get_catgories(text: str, raw_categories: list = [], category_map: list = []) -> list:\n",
    "    if category_map == []:\n",
    "        category_map = load_category_mapping()\n",
    "    if raw_categories == []:\n",
    "        raw_categories = check_categories(text)\n",
    "    \n",
    "    cleaned_categories = clean_categories(raw_categories, category_map)\n",
    "\n",
    "    return cleaned_categories\n",
    "    \n",
    "def get_links(text: str, path: str = 'GoT_files', no_files: bool = True, no_translations: bool = True) -> list:\n",
    "\n",
    "    f = open(os.path.join(path, text), 'r', encoding='utf-8')\n",
    "\n",
    "    if no_files and no_translations:\n",
    "        links = re.findall(r\"\\[{2}(?!\\w{2,5}(?:-\\w{2})?:)(?!File:)(?!Image:).+?\\]{2}\", f.read())\n",
    "    elif no_files:\n",
    "        links = re.findall(r\"\\[{2}(?!File:)(?!Image:).+?\\]{2}\", f.read())\n",
    "    elif no_translations:\n",
    "        links = re.findall(r\"\\[{2}(?!\\w{2,5}(?:-\\w{2})?:).+?\\]{2}\", f.read())\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_links_by_section(file: str, path: str = 'GoT_files', no_files: bool = True, no_translations: bool = True) -> dict:\n",
    "    \"\"\"Extract links from a wiki page, organized by section.\n",
    "    \n",
    "    Args:\n",
    "        file: Filename to read (e.g., 'Page Name.txt')\n",
    "        path: Directory containing the file\n",
    "        no_files: If True, exclude File: and Image: links\n",
    "        no_translations: If True, exclude translation links (e.g., [[de:...]])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with structure:\n",
    "        {\n",
    "            'sections': {\n",
    "                'Section Name': {\n",
    "                    'subsections': {\n",
    "                        'Subsection Name': ['link1', 'link2', ...],\n",
    "                        ...\n",
    "                    },\n",
    "                    'links': ['link1', 'link2', ...]  # Links directly under this section\n",
    "                },\n",
    "                ...\n",
    "            },\n",
    "            'header': ['link1', 'link2', ...],  # Links before first section\n",
    "            'categories': ['category1', 'category2', ...]  # Links after <!--Categories-->\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Build regex pattern for links\n",
    "    if no_files and no_translations:\n",
    "        link_pattern = r\"\\[\\[(?!\\w{2,5}(?:-\\w{2})?:)(?!File:)(?!Image:)(.+?)\\]\\]\"\n",
    "    elif no_files:\n",
    "        link_pattern = r\"\\[\\[(?!File:)(?!Image:)(.+?)\\]\\]\"\n",
    "    elif no_translations:\n",
    "        link_pattern = r\"\\[\\[(?!\\w{2,5}(?:-\\w{2})?:)(.+?)\\]\\]\"\n",
    "    else:\n",
    "        link_pattern = r\"\\[\\[(.+?)\\]\\]\"\n",
    "    \n",
    "    result = {\n",
    "        'sections': {},\n",
    "        'header': [],\n",
    "        'categories': []\n",
    "    }\n",
    "    \n",
    "    # Split text into parts\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    current_section = None\n",
    "    current_subsection = None\n",
    "    in_categories = False\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        # Check if we've reached categories\n",
    "        if '<!--Categories-->' in line:\n",
    "            in_categories = True\n",
    "            continue\n",
    "        \n",
    "        # Check for section headers\n",
    "        section_match = re.match(r'^==(.*?)==\\s*$', line)\n",
    "        subsection_match = re.match(r'^===(.*?)===\\s*$', line)\n",
    "        \n",
    "        if section_match and not subsection_match:\n",
    "            # Main section (== Section ==)\n",
    "            current_section = section_match.group(1).strip()\n",
    "            current_subsection = None\n",
    "            if current_section not in result['sections']:\n",
    "                result['sections'][current_section] = {\n",
    "                    'subsections': {},\n",
    "                    'links': []\n",
    "                }\n",
    "        elif subsection_match:\n",
    "            # Subsection (=== Subsection ===)\n",
    "            current_subsection = subsection_match.group(1).strip()\n",
    "            if current_section:\n",
    "                if current_subsection not in result['sections'][current_section]['subsections']:\n",
    "                    result['sections'][current_section]['subsections'][current_subsection] = []\n",
    "        \n",
    "        # Extract links from this line\n",
    "        matches = re.findall(link_pattern, line)\n",
    "        \n",
    "        for match in matches:\n",
    "            # Clean the link (remove pipe syntax)\n",
    "            if '|' in match:\n",
    "                link = match.split('|')[0]\n",
    "            else:\n",
    "                link = match\n",
    "            \n",
    "            # Skip category links in normal sections\n",
    "            if link.startswith('Category:') or link.startswith('category:'):\n",
    "                if in_categories:\n",
    "                    result['categories'].append(link.replace('Category:', '').replace('category:', ''))\n",
    "                continue\n",
    "            \n",
    "            # Add to appropriate location\n",
    "            if in_categories:\n",
    "                continue  # Skip non-category links after <!--Categories-->\n",
    "            elif current_subsection and current_section:\n",
    "                result['sections'][current_section]['subsections'][current_subsection].append(link)\n",
    "            elif current_section:\n",
    "                result['sections'][current_section]['links'].append(link)\n",
    "            else:\n",
    "                result['header'].append(link)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_character_network(G, weighted=False, top_n=10):\n",
    "    \"\"\"\n",
    "    Plot the largest weakly-connected component of a directed character network.\n",
    "\n",
    "    - Background: light grey\n",
    "    - Nodes: size ~ degree, color ~ in-degree\n",
    "    - Edges: grey; width scaled if weighted=True\n",
    "    - Labels: top_n nodes by degree, white, fontsize=15\n",
    "    - Layout: forceatlas2_layout\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract largest weakly connected component\n",
    "    largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "    G_main = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    # Compute layout\n",
    "    pos = nx.forceatlas2_layout(G_main)\n",
    "\n",
    "    # Degree and in-degree\n",
    "    degrees = dict(G_main.degree())\n",
    "    in_degrees = dict(G_main.in_degree())\n",
    "\n",
    "    # Node sizes\n",
    "    node_sizes = [degrees[n] * 20 for n in G_main.nodes()]\n",
    "\n",
    "    # Node colors (in-degree)\n",
    "    node_color_values = [in_degrees[n] for n in G_main.nodes()]\n",
    "\n",
    "    # Figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 12))\n",
    "    fig.patch.set_facecolor(\"lightgrey\")\n",
    "    ax.set_facecolor(\"lightgrey\")\n",
    "\n",
    "    # Edge widths\n",
    "    if weighted:\n",
    "        weights = [G_main[u][v].get('weight', 1) for u, v in G_main.edges()]\n",
    "        max_w = max(weights)\n",
    "        min_w = min(weights)\n",
    "        edge_widths = [\n",
    "            0.5 + 4.5 * (w - min_w) / (max_w - min_w)\n",
    "            if max_w > min_w else 2\n",
    "            for w in weights\n",
    "        ]\n",
    "    else:\n",
    "        edge_widths = 1\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(\n",
    "        G_main,\n",
    "        pos,\n",
    "        width=edge_widths,\n",
    "        edge_color=\"gray\",\n",
    "        alpha=0.5,\n",
    "        arrows=True if G_main.is_directed() else False,\n",
    "        arrowsize=10,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(\n",
    "        G_main,\n",
    "        pos,\n",
    "        node_size=node_sizes,\n",
    "        node_color=node_color_values,\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.85,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Labels for top nodes\n",
    "    top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    labels = {node: node for node, _ in top_nodes}\n",
    "    nx.draw_networkx_labels(\n",
    "        G_main,\n",
    "        pos,\n",
    "        labels=labels,\n",
    "        font_color='white',\n",
    "        font_size=15,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def extract_aka_names(text: str) -> list:\n",
    "    \"\"\"Extract AKA names from the Character infobox.\n",
    "    \n",
    "    Returns:\n",
    "        List of alternative names for the character (empty list if no AKA names)\n",
    "    \"\"\"\n",
    "    aka_names = []\n",
    "    # Look for | AKA = in the Character template\n",
    "    aka_match = re.search(r'\\|\\s*AKA\\s*=\\s*(.+?)(?=\\n\\||\\n}})', text, re.DOTALL | re.IGNORECASE)\n",
    "    if aka_match:\n",
    "        aka_text = aka_match.group(1).strip()\n",
    "        # Return empty list if AKA field is empty, only whitespace, or starts with | (next field)\n",
    "        if not aka_text or aka_text.startswith('|'):\n",
    "            return []\n",
    "        \n",
    "        # Split by <br> tags (common separator)\n",
    "        parts = re.split(r'<br\\s*/?>', aka_text, flags=re.IGNORECASE)\n",
    "        for part in parts:\n",
    "            # Remove wiki markup like {{Ref|...}} and {{C|...}}\n",
    "            cleaned = re.sub(r'\\{\\{[^}]+\\}\\}', '', part)\n",
    "            # Remove any remaining brackets and extra whitespace\n",
    "            cleaned = re.sub(r'[\\[\\]]', '', cleaned).strip()\n",
    "            # Don't add if it starts with | (indicates we captured the next field)\n",
    "            if cleaned and not cleaned.startswith('|'):\n",
    "                aka_names.append(cleaned)\n",
    "    return aka_names\n",
    "\n",
    "def count_character_mentions(DG_sections, text: str, target_char: str, aka_names: list = None, page_char: str = None, page_aka_names: list = None) -> int:\n",
    "    \"\"\"Count all mentions of a character in text using full name, first name, and AKA names.\n",
    "    Only counts mentions after the first wiki link to the character appears.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to search in\n",
    "        target_char: The full character name to search for\n",
    "        aka_names: List of alternative names for the character\n",
    "        page_char: The character whose page this is (to avoid first-name conflicts)\n",
    "        page_aka_names: List of AKA names for the page character (to avoid conflicts)\n",
    "        \n",
    "    Returns:\n",
    "        Total count of mentions\n",
    "    \"\"\"\n",
    "    if aka_names is None:\n",
    "        aka_names = []\n",
    "    if page_aka_names is None:\n",
    "        page_aka_names = []\n",
    "    \n",
    "    # Special handling for Sam (baby) vs Samwell Tarly\n",
    "    # If target is \"Sam\" and both Sam and Samwell Tarly exist in the graph, only count \"baby\"\n",
    "    # If only Sam exists, count both \"Sam\" and \"baby\"\n",
    "    is_baby_sam = False\n",
    "    if target_char == \"Sam\":\n",
    "        # Check if Samwell Tarly exists in DG_sections (if it exists, this Sam is the baby)\n",
    "        if 'Samwell Tarly' in DG_sections.nodes():\n",
    "            is_baby_sam = True\n",
    "    \n",
    "    # Titles that shouldn't trigger first-name counting\n",
    "    title_prefixes = ['Lord', 'Lady', 'King', 'Queen', 'Ser', 'Maester', 'Grand Maester',\n",
    "                      'Prince', 'Princess', 'Night', 'High']\n",
    "    \n",
    "    # Remove only specific infobox fields that shouldn't be counted\n",
    "    # Keep relationship fields like Father, Mother, Siblings, Spouse, Lovers, Issue\n",
    "    fields_to_remove = [\n",
    "        'Title', 'Type', 'Image', 'Birth', 'Death', 'House', 'Affiliation', 'Titles', \n",
    "        'AKA', 'Culture', 'Religion', 'Arms', 'Series', 'Season', 'Seasons', \n",
    "        'Appeared in', 'Appearances', 'First', 'Last', 'DeathEp', 'Actor', \n",
    "        'Status', 'Origin', 'Allegiance'\n",
    "    ]\n",
    "    \n",
    "    # Find the end of the infobox to start searching after it\n",
    "    infobox_end = 0\n",
    "    infobox_match = re.search(r'\\{\\{Character.*?\\n\\}\\}', text, re.DOTALL | re.IGNORECASE)\n",
    "    if infobox_match:\n",
    "        infobox_end = infobox_match.end()\n",
    "    \n",
    "    # Look for first wiki link to target character (after infobox)\n",
    "    # Pattern: [[target_char]] or [[target_char|display]]\n",
    "    link_patterns = [\n",
    "        r'\\[\\[' + re.escape(target_char) + r'(?:\\|[^\\]]+)?\\]\\]',\n",
    "    ]\n",
    "    \n",
    "    first_link_pos = None\n",
    "    for link_pattern in link_patterns:\n",
    "        match = re.search(link_pattern, text[infobox_end:], re.IGNORECASE)\n",
    "        if match:\n",
    "            first_link_pos = infobox_end + match.start()\n",
    "            break\n",
    "    \n",
    "    # If no link found, return 0 (character not mentioned on this page)\n",
    "    if first_link_pos is None:\n",
    "        return 0\n",
    "    \n",
    "    # Start count at 1 to include the first link itself as a mention\n",
    "    total_count = 1\n",
    "    \n",
    "    # Only process text after the first link for additional mentions\n",
    "    text_to_process = text[first_link_pos:]\n",
    "    \n",
    "    # Remove specific infobox fields from the text we're processing\n",
    "    clean_text = text_to_process\n",
    "    for field in fields_to_remove:\n",
    "        pattern = r'\\|\\s*' + re.escape(field) + r'\\s*=.*?(?=\\n\\s*\\||}})'\n",
    "        clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Remove other wiki templates like {{Ref|...}}, {{C|...}}, etc.\n",
    "    clean_text = re.sub(r'\\{\\{(?!.*?\\[\\[)([^}]+)\\}\\}', '', clean_text)\n",
    "    \n",
    "    # Remove [[...]] links but keep the display text\n",
    "    clean_text = re.sub(r'\\[\\[([^\\]|]+?)(?:\\|([^\\]]+?))?\\]\\]', lambda m: m.group(2) if m.group(2) else m.group(1), clean_text)\n",
    "    \n",
    "    # Note: total_count already initialized to 1 (for the first link)\n",
    "    \n",
    "    # Special case: if this is baby Sam (when Samwell Tarly exists), only count \"baby\"\n",
    "    if is_baby_sam:\n",
    "        # Only count mentions of \"baby\" (case-insensitive for common nouns)\n",
    "        pattern = r'\\b' + re.escape(\"baby\") + r\"(?:'s)?\\b\"\n",
    "        for match in re.finditer(pattern, clean_text, re.IGNORECASE):\n",
    "            total_count += 1\n",
    "        return total_count\n",
    "    \n",
    "    # Determine first name and whether to use it\n",
    "    first_name = None\n",
    "    use_first_name = False\n",
    "    \n",
    "    if ' ' in target_char:\n",
    "        first_name = target_char.split()[0]\n",
    "        \n",
    "        # Check if first name is a title that shouldn't be counted\n",
    "        if first_name not in title_prefixes:\n",
    "            # Check if page character has same first name (e.g., Jon Snow page searching for Jon Roxton)\n",
    "            if page_char and ' ' in page_char:\n",
    "                page_first_name = page_char.split()[0]\n",
    "                if page_first_name != first_name:\n",
    "                    use_first_name = True\n",
    "            else:\n",
    "                use_first_name = True\n",
    "    \n",
    "    # Find all positions where the full name matches\n",
    "    full_name_positions = set()\n",
    "    if target_char:\n",
    "        # For single-word names, use case-sensitive matching to avoid common words\n",
    "        # For multi-word names, require first letter case match but rest can be case-insensitive\n",
    "        if ' ' not in target_char:\n",
    "            # Single word name - fully case-sensitive (e.g., \"Will\" won't match \"will\")\n",
    "            pattern = r'\\b' + re.escape(target_char) + r\"(?:'s)?\\b\"\n",
    "            for match in re.finditer(pattern, clean_text):\n",
    "                full_name_positions.update(range(match.start(), match.end()))\n",
    "                total_count += 1\n",
    "        else:\n",
    "            # Multi-word name - first letter must match case, rest case-insensitive\n",
    "            first_letter = re.escape(target_char[0])\n",
    "            rest_of_name = re.escape(target_char[1:])\n",
    "            pattern = r'\\b' + first_letter + rest_of_name + r\"(?:'s)?\\b\"\n",
    "            for match in re.finditer(pattern, clean_text, re.IGNORECASE):\n",
    "                full_name_positions.update(range(match.start(), match.end()))\n",
    "                total_count += 1\n",
    "    \n",
    "    # Count first name matches (if applicable), but skip overlaps with full name\n",
    "    # Also check if first name matches any of the page character's AKA names\n",
    "    if use_first_name and first_name:\n",
    "        # Check if this first name is one of the page character's AKA names\n",
    "        first_name_is_page_aka = any(first_name.lower() == aka.lower() for aka in page_aka_names)\n",
    "        \n",
    "        if not first_name_is_page_aka:\n",
    "            # Use case-sensitive matching for first names to avoid common words\n",
    "            pattern = r'\\b' + re.escape(first_name) + r\"(?:'s)?\\b\"\n",
    "            for match in re.finditer(pattern, clean_text):\n",
    "                if match.start() not in full_name_positions:\n",
    "                    total_count += 1\n",
    "                    # Mark these positions as used too\n",
    "                    for pos in range(match.start(), match.end()):\n",
    "                        full_name_positions.add(pos)\n",
    "    \n",
    "    # Count AKA name matches, skipping overlaps\n",
    "    # Also check if AKA name matches page character's AKA names\n",
    "    for aka_name in aka_names:\n",
    "        # Check if this AKA name is one of the page character's AKA names\n",
    "        aka_is_page_aka = any(aka_name.lower() == page_aka.lower() for page_aka in page_aka_names)\n",
    "        \n",
    "        if not aka_is_page_aka and aka_name:\n",
    "            # Use case-sensitive matching for AKA names to avoid common words\n",
    "            pattern = r'\\b' + re.escape(aka_name) + r\"(?:'s)?\\b\"\n",
    "            for match in re.finditer(pattern, clean_text):\n",
    "                if match.start() not in full_name_positions:\n",
    "                    total_count += 1\n",
    "    \n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8b81d",
   "metadata": {},
   "source": [
    "# Downloading the wiki pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "530660c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page titles from the wiki...\n",
      "  Fetched 500 page titles...\n",
      "Downloading pages 0 to 1 (0 pages)\n",
      "\n",
      "============================================================\n",
      "DOWNLOAD COMPLETE!\n",
      "============================================================\n",
      "Successfully saved: 0 pages\n",
      "Redirects (skipped): 0 pages\n",
      "Failed: 0 pages\n",
      "\n",
      "Files saved in: c:\\Users\\sofie\\OneDrive - Danmarks Tekniske Universitet\\Kandidat\\1. semester\\02805 - Social graphs and interactions\\Assignments\\game_of_social_graphs_and_thrones\\GoT_files_final\n"
     ]
    }
   ],
   "source": [
    "# Chose an interval of pages, that haven't already been downloaded\n",
    "# ==== CONFIGURATION ====\n",
    "START_PAGE = 0      # Start from this page number\n",
    "END_PAGE = 1 # ends around 8000, good to download in batches      # End at this page number (inclusive)\n",
    "OUTPUT_FOLDER = \"GoT_files_final\"\n",
    "\n",
    "\n",
    "# HUSK AT BRUGE DIT NAVN!!!!!!\n",
    "# The script will write worker-specific files like fetched_pages_<name>.txt\n",
    "# WORKER_NAME = \"mathias\"\n",
    "# WORKER_NAME = \"nikolai\"\n",
    "WORKER_NAME = \"sofie\" \n",
    "# =======================\n",
    "\n",
    "\n",
    "# Get all page titles first\n",
    "print(f\"Fetching page titles from the wiki...\")\n",
    "url = \"https://gameofthrones.fandom.com/api.php\"\n",
    "all_page_titles = []\n",
    "apcontinue = None\n",
    "\n",
    "while len(all_page_titles) < END_PAGE:\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"aplimit\": \"500\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    if apcontinue:\n",
    "        params[\"apcontinue\"] = apcontinue\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    pages = [page['title'] for page in data['query']['allpages']]\n",
    "    all_page_titles.extend(pages)\n",
    "    \n",
    "    print(f\"  Fetched {len(all_page_titles)} page titles...\")\n",
    "    \n",
    "    if 'continue' in data and 'apcontinue' in data['continue']:\n",
    "        apcontinue = data['continue']['apcontinue']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Select the page range\n",
    "pages_to_download = all_page_titles[START_PAGE-1:END_PAGE]\n",
    "total_pages = len(pages_to_download)\n",
    "\n",
    "print(f\"Downloading pages {START_PAGE} to {END_PAGE} ({total_pages} pages)\")\n",
    "\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "redirects = 0\n",
    "failed_pages = []\n",
    "\n",
    "# Prepare fetched pages tracking file inside data_handling folder\n",
    "DATA_HANDLING_FOLDER = \"data_handling_final\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(DATA_HANDLING_FOLDER, exist_ok=True)\n",
    "# Determine worker-specific filenames\n",
    "if not WORKER_NAME:\n",
    "    # Try environment fallbacks if WORKER_NAME not set\n",
    "    WORKER_NAME = os.getenv('USER') or os.getenv('USERNAME') or 'worker'\n",
    "safe_worker = WORKER_NAME.replace(' ', '_')\n",
    "worker_fetched_file = os.path.join(DATA_HANDLING_FOLDER, f\"fetched_pages_{safe_worker}.txt\")\n",
    "worker_redirects_file = os.path.join(DATA_HANDLING_FOLDER, f\"redirects_{safe_worker}.txt\")\n",
    "worker_failed_file = os.path.join(DATA_HANDLING_FOLDER, f\"failed_pages_{safe_worker}.txt\")\n",
    "# Build processed set by reading all workers' fetched files so we avoid re-downloading what others already fetched\n",
    "processed = set()\n",
    "for path in glob.glob(os.path.join(DATA_HANDLING_FOLDER, 'fetched_pages_*.txt')):\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    processed.add(int(line))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        # ignore unreadable files\n",
    "        pass\n",
    "\n",
    "for i, page_title in enumerate(pages_to_download, 1):\n",
    "    actual_page_num = START_PAGE + i - 1\n",
    "\n",
    "    # Skip pages that were already processed (downloaded or redirected)\n",
    "    if actual_page_num in processed:\n",
    "        print(f\"[{actual_page_num}/{END_PAGE}] Skipping (already fetched)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{actual_page_num}/{END_PAGE}] Downloading: {page_title}...\\n\", end=\" \")\n",
    "    \n",
    "    wikitext = get_page_wikitext(page_title)\n",
    "    \n",
    "    if wikitext:\n",
    "        # Check if this is a redirect page (case-insensitive)\n",
    "        if wikitext.strip().upper().startswith(\"#REDIRECT\"):\n",
    "            redirects += 1\n",
    "            # Log the redirect to a worker-specific file inside the output folder\n",
    "            with open(worker_redirects_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"{page_title} -> {wikitext.strip()}\\n\")\n",
    "            # Record the interval number for this redirected page to this worker's fetched file\n",
    "            with open(worker_fetched_file, \"a\", encoding=\"utf-8\") as fnum:\n",
    "                fnum.write(f\"{actual_page_num}\\n\")\n",
    "            processed.add(actual_page_num)\n",
    "        else:\n",
    "            filepath = save_wikitext_to_file(page_title, wikitext, OUTPUT_FOLDER)\n",
    "            successful += 1\n",
    "            # Record the interval number for the successfully downloaded page to this worker's file\n",
    "            with open(worker_fetched_file, \"a\", encoding=\"utf-8\") as fnum:\n",
    "                fnum.write(f\"{actual_page_num}\\n\")\n",
    "            processed.add(actual_page_num)\n",
    "    else:\n",
    "        failed += 1\n",
    "        failed_pages.append(page_title)\n",
    "        with open(worker_failed_file, \"a\", encoding=\"utf-8\") as ffail:\n",
    "            ffail.write(f\"{page_title}\\n\")\n",
    "        print(\"âœ— Failed\")\n",
    "    \n",
    "    # Be polite to the server\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DOWNLOAD COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully saved: {successful} pages\")\n",
    "print(f\"Redirects (skipped): {redirects} pages\")\n",
    "print(f\"Failed: {failed} pages\")\n",
    "if failed_pages:\n",
    "    print(f\"\\nFailed pages:\")\n",
    "    for page in failed_pages[:10]:\n",
    "        print(f\"  - {page}\")\n",
    "    if len(failed_pages) > 10:\n",
    "        print(f\"  ... and {len(failed_pages) - 10} more\")\n",
    "if redirects > 0:\n",
    "    print(f\"\\nRedirects logged to: {worker_redirects_file}\")\n",
    "print(f\"\\nFiles saved in: {os.path.abspath(OUTPUT_FOLDER)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c364f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files containing \"#In the books\" to Doubles subfolder\n",
    "\n",
    "source_folder = 'GoT_files_final'\n",
    "target_folder = os.path.join('GoT_files_final', 'Doubles')\n",
    "\n",
    "# Create target folder if it doesn't exist\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "# Get all files in source folder\n",
    "files = os.listdir(source_folder)\n",
    "\n",
    "moved_count = 0\n",
    "for filename in files:\n",
    "    if '#In the books' in filename:\n",
    "        source_path = os.path.join(source_folder, filename)\n",
    "        target_path = os.path.join(target_folder, filename)\n",
    "        \n",
    "        # Only move if it's a file (not a directory)\n",
    "        if os.path.isfile(source_path):\n",
    "            shutil.move(source_path, target_path)\n",
    "            moved_count += 1\n",
    "            print(f\"Moved: {filename}\")\n",
    "\n",
    "print(f\"\\nTotal files moved: {moved_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086b831",
   "metadata": {},
   "source": [
    "# Updating the graphs with links and sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13707080",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'GoT_files_final'\n",
    "\n",
    "pages = os.listdir(path)\n",
    "\n",
    "page_texts = load_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "333b3424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5672 nodes\n"
     ]
    }
   ],
   "source": [
    "# Build graph with section information\n",
    "DG_sections = nx.DiGraph()\n",
    "\n",
    "# First pass: add all nodes\n",
    "for page in page_texts.keys():\n",
    "    DG_sections.add_node(page)\n",
    "\n",
    "print(f\"Added {DG_sections.number_of_nodes()} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a85ac3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = load_category_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d79837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 5672 nodes and 198768 edges\n"
     ]
    }
   ],
   "source": [
    "# Character categories to check\n",
    "character_categories = {\n",
    "    'Individuals from Game of Thrones',\n",
    "    'Individuals from House of the Dragon',\n",
    "    'Individuals from A Knight of the Seven Kingdoms',\n",
    "    'Individuals appearing in Game of Thrones',\n",
    "    'Individuals appearing in House of the Dragon',\n",
    "    'Individuals appearing in A Knight of the Seven Kingdoms',\n",
    "}\n",
    "\n",
    "for page in page_texts.keys():\n",
    "    file = page + '.txt'\n",
    "    section_data = get_links_by_section(file)\n",
    "    \n",
    "    # Store categories as node attribute\n",
    "    cleaned_categories = clean_categories(section_data['categories'], category_map)\n",
    "    DG_sections.nodes[page]['categories'] = cleaned_categories\n",
    "    \n",
    "    # Extract and store AKA names only for character nodes\n",
    "    if any(cat in character_categories for cat in cleaned_categories):\n",
    "        page_text = page_texts[page]\n",
    "        aka_names = extract_aka_names(page_text)\n",
    "        if aka_names:\n",
    "            DG_sections.nodes[page]['also_known_as'] = aka_names\n",
    "    \n",
    "    # Process header links\n",
    "    for link in section_data['header']:\n",
    "        if link in DG_sections.nodes():\n",
    "            # Check if edge already exists\n",
    "            if DG_sections.has_edge(page, link):\n",
    "                # Add to existing sections list\n",
    "                DG_sections[page][link]['sections'].append('header')\n",
    "            else:\n",
    "                # Create new edge with sections list\n",
    "                DG_sections.add_edge(page, link, sections=['header'])\n",
    "    \n",
    "    # Process section links\n",
    "    for section_name, section_content in section_data['sections'].items():\n",
    "        # Links directly under section\n",
    "        for link in section_content['links']:\n",
    "            if link in DG_sections.nodes():\n",
    "                section_label = section_name\n",
    "                if DG_sections.has_edge(page, link):\n",
    "                    DG_sections[page][link]['sections'].append(section_label)\n",
    "                else:\n",
    "                    DG_sections.add_edge(page, link, sections=[section_label])\n",
    "        \n",
    "        # Links in subsections\n",
    "        for subsection_name, links in section_content['subsections'].items():\n",
    "            for link in links:\n",
    "                if link in DG_sections.nodes():\n",
    "                    section_label = f\"{section_name} > {subsection_name}\"\n",
    "                    if DG_sections.has_edge(page, link):\n",
    "                        DG_sections[page][link]['sections'].append(section_label)\n",
    "                    else:\n",
    "                        DG_sections.add_edge(page, link, sections=[section_label])\n",
    "    \n",
    "    # Process categories as edges\n",
    "    for cat in cleaned_categories:\n",
    "        if cat in DG_sections.nodes():\n",
    "            if DG_sections.has_edge(page, cat):\n",
    "                DG_sections[page][cat]['sections'].append('categories')\n",
    "            else:\n",
    "                DG_sections.add_edge(page, cat, sections=['categories'])\n",
    "\n",
    "print(f\"Graph has {DG_sections.number_of_nodes()} nodes and {DG_sections.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b07daf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('graphs_final', exist_ok=True)\n",
    "\n",
    "# Save graphs\n",
    "with open(os.path.join('graphs_final', 'dg_sections.pickle'), 'wb') as f:\n",
    "    pickle.dump(DG_sections, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6bec7",
   "metadata": {},
   "source": [
    "# Making the character network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0752247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 847 character nodes based on category attributes\n"
     ]
    }
   ],
   "source": [
    "# Identify character nodes using category attributes\n",
    "character_categories = set([\n",
    "    'Individuals from Game of Thrones',\n",
    "    'Individuals from House of the Dragon',\n",
    "    'Individuals appearing in Game of Thrones',\n",
    "    'Individuals appearing in House of the Dragon',\n",
    "])\n",
    "\n",
    "# Find all character nodes based on their category attributes\n",
    "character_nodes = set()\n",
    "for node in DG_sections.nodes():\n",
    "    node_cats = DG_sections.nodes[node].get('categories', [])\n",
    "    \n",
    "    # Check if node has any character categories\n",
    "    if any(cat in character_categories for cat in node_cats):\n",
    "        character_nodes.add(node)\n",
    "\n",
    "print(f\"Found {len(character_nodes)} character nodes based on category attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c3fbd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted character network complete!\n",
      "  Nodes: 847\n",
      "  Edges: 10601\n"
     ]
    }
   ],
   "source": [
    "# Build character network by extracting character-to-character edges from DG_sections\n",
    "# Create subgraph with only character nodes\n",
    "DG_characters = DG_sections.subgraph(character_nodes).copy()\n",
    "\n",
    "# Filter to keep only edges between characters (exclude category edges and self-loops)\n",
    "edges_to_remove = []\n",
    "for source, target in DG_characters.edges():\n",
    "    # Remove edge if it's a category edge or self-loop\n",
    "    if source == target or DG_characters[source][target].get('sections') == ['categories']:\n",
    "        edges_to_remove.append((source, target))\n",
    "\n",
    "DG_characters.remove_edges_from(edges_to_remove)\n",
    "\n",
    "# Update node attributes to only include 'type' and 'also_known_as'\n",
    "for node in DG_characters.nodes():\n",
    "    # Keep only type and also_known_as attributes\n",
    "    node_data = DG_characters.nodes[node]\n",
    "    aka = DG_sections.nodes[node].get('also_known_as')\n",
    "    \n",
    "    # Clear all attributes and set only what we need\n",
    "    node_data.clear()\n",
    "    node_data['type'] = 'character'\n",
    "    if aka:  # Only add if not empty\n",
    "        node_data['also_known_as'] = aka\n",
    "\n",
    "print(f\"Unweighted character network complete!\")\n",
    "print(f\"  Nodes: {DG_characters.number_of_nodes()}\")\n",
    "print(f\"  Edges: {DG_characters.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a42142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DG_characters to graphs_final/dg_characters.pickle\n"
     ]
    }
   ],
   "source": [
    "# Save the character network\n",
    "with open(os.path.join('graphs_final', 'dg_characters.pickle'), 'wb') as f:\n",
    "    pickle.dump(DG_characters, f)\n",
    "\n",
    "print(\"Saved DG_characters to graphs_final/dg_characters.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea80f7f",
   "metadata": {},
   "source": [
    "# Creating the weighted character network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d2bdd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied unweighted graph: 847 nodes, 10601 edges\n"
     ]
    }
   ],
   "source": [
    "# Create weighted graph as a copy of the unweighted graph\n",
    "DGW_characters = DG_characters.copy()\n",
    "\n",
    "print(f\"Copied unweighted graph: {DGW_characters.number_of_nodes()} nodes, {DGW_characters.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd2bc89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding weights to character network...\n",
      "Processing 10601 edges...\n",
      "\n",
      "Weighted character network complete!\n",
      "  Nodes: 847\n",
      "  Edges: 10601\n"
     ]
    }
   ],
   "source": [
    "# Add weights to existing edges by counting mentions\n",
    "# We only need to check edges that already exist in the graph\n",
    "\n",
    "print(\"Adding weights to character network...\")\n",
    "print(f\"Processing {DGW_characters.number_of_edges()} edges...\")\n",
    "\n",
    "edges_processed = 0\n",
    "\n",
    "for source_char, target_char in DGW_characters.edges():    \n",
    "    # Get the text for the source character's page\n",
    "    if source_char not in page_texts:\n",
    "        # If page doesn't exist, set weight to 1 (for the link itself)\n",
    "        DGW_characters[source_char][target_char]['weight'] = 1\n",
    "        continue\n",
    "    \n",
    "    source_text = page_texts[source_char]\n",
    "    \n",
    "    # Get AKA names for target character\n",
    "    aka_names = DG_sections.nodes[target_char].get('also_known_as', [])\n",
    "    \n",
    "    # Get AKA names for page character (to avoid counting page char's own AKA names)\n",
    "    page_aka_names = DG_sections.nodes[source_char].get('also_known_as', [])\n",
    "    \n",
    "    # Count mentions (this will be at least 1 because the link exists)\n",
    "    mention_count = count_character_mentions(DG_sections,source_text, target_char, aka_names, \n",
    "                                            page_char=source_char, page_aka_names=page_aka_names)\n",
    "    \n",
    "    # Add weight to edge\n",
    "    DGW_characters[source_char][target_char]['weight'] = mention_count\n",
    "\n",
    "print(f\"\\nWeighted character network complete!\")\n",
    "print(f\"  Nodes: {DGW_characters.number_of_nodes()}\")\n",
    "print(f\"  Edges: {DGW_characters.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73fb8c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DGW_characters to graphs_final/dgw_characters.pickle\n"
     ]
    }
   ],
   "source": [
    "# Save the weighted character network\n",
    "with open(os.path.join('graphs_final', 'dgw_characters.pickle'), 'wb') as f:\n",
    "    pickle.dump(DGW_characters, f)\n",
    "\n",
    "print(\"Saved DGW_characters to graphs_final/dgw_characters.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f98609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
